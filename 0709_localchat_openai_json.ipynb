{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install ipykernel\n",
    "# ! pip install gradio\n",
    "# ! pip install -U langchain-openai\n",
    "# ! pip install ipywidgets --upgrade\n",
    "# ! pip install --upgrade langchain\n",
    "# ! pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "import gradio as gr\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ.get (\"openai_api_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Retrieve the list of available models\n",
    "# client = OpenAI(api_key = OPENAI_API_KEY)\n",
    "# models = client.models.list()\n",
    "# for model in models.data:\n",
    "#     print(model.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langchain opeai ëª¨ë¸ ì´ˆê¸°í™”\n",
    "TEMPERATURE = 0\n",
    "MODEL_NAME = 'gpt-4o'\n",
    "llm = ChatOpenAI(temperature=TEMPERATURE, model=MODEL_NAME)\n",
    "\n",
    "# ì „ì²´ ëŒ€í™” ê¸°ë¡ì„ ìœ ì§€í•  ì „ì—­ ë³€ìˆ˜\n",
    "global_chat_history = {\n",
    "    \"meta\": {\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"model\": MODEL_NAME\n",
    "    },\n",
    "    \"prompts\": []\n",
    "}\n",
    "\n",
    "# í˜„ì¬ ì‹œê°„ì„ yyyymmdd_hhmmss í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜\n",
    "def get_current_time():\n",
    "    return datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# ë¡œì»¬ì— ëŒ€í™” ê¸°ë¡ ì €ì¥ í•¨ìˆ˜ (ë²„íŠ¼ í´ë¦­ ì‹œ í˜¸ì¶œ)\n",
    "def save_chat_history_to_local(session_id):\n",
    "    output_directory = \"_output_OpenAI\"\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    timestamp = get_current_time()\n",
    "    file_name = f\"{timestamp}_{session_id}.json\"\n",
    "    file_path = os.path.join(output_directory, file_name)\n",
    "\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(global_chat_history, file, ensure_ascii=False, indent=4)\n",
    "    print(f\"Chat history saved to {file_path}\")\n",
    "    return f\"Chat history saved to {file_path}\"\n",
    "\n",
    "# Gradioì—ì„œ ìœ ì €ì˜ ì§ˆë¬¸ê³¼ ì±„íŒ…ê¸°ë¡ì„ ë°›ì•„ GPT-4ì˜ ëŒ€ë‹µì„ ë¦¬í„´í•˜ëŠ” í•¨ìˆ˜\n",
    "def response(message, history, system_message):\n",
    "    history_langchain_format = [SystemMessage(content=system_message)]\n",
    "    for human, ai in history:\n",
    "        history_langchain_format.append(HumanMessage(content=human))\n",
    "        history_langchain_format.append(AIMessage(content=ai))\n",
    "    history_langchain_format.append(HumanMessage(content=message))\n",
    "    gpt_response = llm.invoke(history_langchain_format)\n",
    "    user_message_time = get_current_time()\n",
    "    bot_response_time = get_current_time()\n",
    "    \n",
    "    # ê¸€ë¡œë²Œ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€\n",
    "    if not global_chat_history[\"prompts\"] or global_chat_history[\"prompts\"][-1][\"prompt\"] != system_message:\n",
    "        global_chat_history[\"prompts\"].append({\n",
    "            \"prompt\": system_message,\n",
    "            \"history\": []\n",
    "        })\n",
    "    \n",
    "    global_chat_history[\"prompts\"][-1][\"history\"].append({\n",
    "        \"user_message\": message,\n",
    "        \"user_message_time\": user_message_time,\n",
    "        \"bot_response\": gpt_response.content,\n",
    "        \"bot_response_time\": bot_response_time\n",
    "    })\n",
    "    \n",
    "    return gpt_response.content\n",
    "\n",
    "# Gradio ì„¸ì…˜ ê´€ë¦¬ í•¨ìˆ˜\n",
    "def generate_session_id():\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "# Gradio UI ì„¤ì • ë° ì‹¤í–‰\n",
    "with gr.Blocks() as demo:\n",
    "    session_id_input = gr.Textbox(value=generate_session_id(), label=\"Session ID\", placeholder=\"Enter your session ID\")\n",
    "    chatbot = gr.Chatbot(height=500)\n",
    "    msg = gr.Textbox(placeholder=\"User Messageë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”\", container=False, scale=7)\n",
    "    additional_input = gr.Textbox(\"\", label=\"System Prompt\", placeholder=\"\")\n",
    "    save_button = gr.Button(\"ëŒ€í™” ê¸°ë¡ ì €ì¥\")\n",
    "\n",
    "    state = gr.State(value=\"\")  # ì´ˆê¸° ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒíƒœ\n",
    "\n",
    "    def submit_message(message, history, session_id, system_message):\n",
    "        response_text = response(message, history, system_message)\n",
    "        history.append((message, response_text))\n",
    "        return history, \"\"\n",
    "    \n",
    "    def delete_last_chat(chat_history):\n",
    "        if len(chat_history) > 0:\n",
    "            chat_history.pop()\n",
    "        return chat_history\n",
    "\n",
    "    def update_system_message(new_prompt):\n",
    "        state.value = new_prompt\n",
    "        return new_prompt\n",
    "\n",
    "    def save_chat(session_id):\n",
    "        return save_chat_history_to_local(session_id)\n",
    "\n",
    "    msg.submit(submit_message, [msg, chatbot, session_id_input, state], [chatbot, msg])\n",
    "    additional_input.change(update_system_message, inputs=additional_input, outputs=state)\n",
    "    save_button.click(save_chat, inputs=[session_id_input], outputs=None)\n",
    "\n",
    "    gr.Button(\"ì…ë ¥í•˜ê¸° â†©\").click(fn=submit_message, inputs=[msg, chatbot, session_id_input, state], outputs=[chatbot, msg])\n",
    "    gr.Button(\"ì´ì „ì±— ì‚­ì œ âŒ\").click(fn=delete_last_chat, inputs=chatbot, outputs=chatbot)\n",
    "    gr.Button(\"ì „ì±— ì‚­ì œ ğŸ’«\").click(fn=lambda: [], inputs=None, outputs=chatbot)\n",
    "\n",
    "demo.launch(share=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
